{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe54735-1c2f-4ab6-a8e0-19e743a135a3",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.pieriantraining.com/\" ><img src=\"PTCenteredPurple.png\" alt=\"Pierian Training Logo\" /></a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeca1d-5190-4346-8599-9222dac6b4c1",
   "metadata": {},
   "source": [
    "# LLM Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e59af6",
   "metadata": {},
   "source": [
    "In this notebook we'll walk through the process of fine-tuning one of OpenAI's advanced language models on the [MTSamples](https://mtsamples.com/) dataset. The objective is to enhance the model's proficiency in understanding and generating medical content, making it a valuable tool for healthcare professionals, researchers, and students.\n",
    "\n",
    "Large language models have shown remarkable abilities to understand, generate, and even creatively engage with a wide range of topics. However, when it comes to medical data and other specialized fields, their proficiency can sometimes be less than optimal due to a variety of reasons:\n",
    "\n",
    "1. Training Data:<br />\n",
    "LLMs are trained on unimaginable amount of data from the internet. However, due to e.g. HIPAA privacy regulations, the percentage of high-quality,  medical content on the internet is limited compared to other topics. Therefore, LLMs might not have been exposed to as much specialized medical knowledge during their training.\n",
    "\n",
    "2. Complexity and Specificity:<br />\n",
    "Medical data and literature often contain highly specialized terms, concepts, and relationships that are complex. Proficiency in this domain requires not only an understanding of the terms but also the context in which they are used. LLMs can sometimes misinterpret or oversimplify these intricate concepts.\n",
    "\n",
    "3. Generalization vs. Specialization:<br />\n",
    "Large Language Models are designed to be generalists, capable of addressing a wide range of topics. While they can generate information on many subjects, they might not always match the depth and accuracy of a model or system specifically designed and trained for medical data.\n",
    "\n",
    "\n",
    "**To this end, the goal of this lecture is to fine tune {INSERT MODEL HERE} on medical reports in order to classify medical reports based on the underlying specialty !**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534fd55e",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "1. **Exploring the MTSamples Dataset:**<br />\n",
    "    - We'll begin by exploring the dataset and its structure.\n",
    "2. **Preprocessing:** \n",
    "    - To prepare for the fine-tuning process, we need to process the dataset into a specific format\n",
    "3. **Fine-Tuning Process:** \n",
    "    - A step-by-step guide to fine-tuning the model, including setting up the right hyperparameters.\n",
    "4. **Evaluation:** \n",
    "    - After training, we'll evaluate our fine-tuned model's performance on medical transcriptions.\n",
    "5. **Applications & Use-Cases:** \n",
    "    - Brief insights into the myriad of ways this fine-tuned model can be utilized in the healthcare domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9621993",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e97ac7",
   "metadata": {},
   "source": [
    "# Exploring the MTSamples Dataset\n",
    "\n",
    "The dataset was originally obtained from [kaggle](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions/).\n",
    "Note that we already removed all unncessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d3261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6d3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_reports = pd.read_csv(\"reports.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f3e37",
   "metadata": {},
   "source": [
    "### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_reports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca84368",
   "metadata": {},
   "source": [
    "We can see, that the dataset consists of the patient's report and the corresponding medical specialty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca0d6f",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Let's check the dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad881bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_reports.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a2a3c",
   "metadata": {},
   "source": [
    "We can see that the number of medical_specialty differs from the reports. Let's remove the entries with the missing reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where 'report' is missing\n",
    "medical_reports.dropna(subset=['report'], inplace=True)\n",
    "medical_reports.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c8ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fill the na values\n",
    "# medical_reports.fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = medical_reports.groupby(\"medical_specialty\").sample(110, random_state=42) # Sample 110 items from each class\n",
    "grouped_data['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d856dcb",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Before we inspect the dataset in more detail, let's at first create the train-val-test split\n",
    "Let's select 5 samples out of each class for validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9507a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = medical_reports.groupby(\"medical_specialty\").sample(110, random_state=42) # Sample 110 items from each class\n",
    "\n",
    "val_test_data = grouped_data.groupby(\"medical_specialty\").sample(10, random_state=42)  # sample 10 items from the above data\n",
    "val = val_test_data.groupby(\"medical_specialty\").head(5) # Take the first 5 of each class\n",
    "test = val_test_data.groupby(\"medical_specialty\").tail(5) # Take the last 5 of each class\n",
    "\n",
    "train = grouped_data[~grouped_data.index.isin(val_test_data.index)] # Take the remaining ones for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12aa43",
   "metadata": {},
   "source": [
    "### Dataset Statistics\n",
    "Let's explore the dataset to provide some basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Number of unique medical specialties\n",
    "print(f\"Number of unique medical specialties: {train['medical_specialty'].nunique()}\")\n",
    "\n",
    "# 2. Distribution of reports across different medical specialties\n",
    "print(\"\\nDistribution of reports across medical specialties:\")\n",
    "print(train['medical_specialty'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd3c8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Average, minimum, and maximum report length (in tokens, not words).\n",
    "# This is important due to token limitations and also to estimate the price.\n",
    "# Let's calculate the tokens for OpenAI's cheapest model, babbage-002\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\n",
    "    (https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # encoding for currently all models\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Para la crema pastelera:250 mililitros de leche entera,75 gramos de azúcar,2 yemas de huevo M,25 gramos de maicena (harina de maíz refinada),\n",
    "Para la tarta:1 lámina de hojaldre,2 manzanas,10 gramos de azúcar moreno,2 cucharadas de mermeladas de albaricoque\n",
    "Comenzamos por la crema pastelera, ya que necesitamos que esté fría antes de montar la tarta. Para ello, calentamos en un cazo 250 ml de leche entera con 40 gramos de azúcar, removiendo para que se disuelva el azúcar.\n",
    "Mientras, mezclamos en un cuenco pequeño 2 yemas de huevo M con el resto del azúcar (35 g) y cuando esté mezclado agregamos 25 gramos de maicena. Vertemos un poco de la leche caliente, mezclamos bien y vertemos de nuevo en el cazo, removiendo para que espese. \n",
    "Tapamos con film transparente tocando la crema (para que no se seque) y dejamos enfriar completamente.\n",
    "Estiramos 1 lámina de masa de hojaldre sobre el molde elegido para la tarta y forrado con papel de horno y pinchamos la base con un tenedor. Desechamos los sobrantes de masa y reservamos en la nevera mientras precalentamos el horno a 200°C.\n",
    "Lavamos 2 manzanas, desechamos el corazón y las cortamos finas en láminas.\n",
    "Repartimos la crema pastelera fría sobre la base. Cubrimos con las manzanas cortadas. Espolvoreamos 10 gramos de azúcar moreno por encima.\n",
    "Horneamos la tarta de manzana 20 minutos, retiramos del horno y en caliente, pintamos la superficie con 2 cucharadas de mermelada de albaricoque. Dejamos reposar 10 minutos, desmoldamos y dejamos enfriar del todo encima de una rejilla.\n",
    "¡Y listo! Ya solo queda disfrutar de esta deliciosa tarta de manzana con crema pastelera.\"\"\"\n",
    "\n",
    "\n",
    "a = num_tokens_from_string(text)\n",
    "print(f\"tokens: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61807ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lengths = train['report'].apply(num_tokens_from_string)\n",
    "report_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_lengths = train['report'].apply(lambda x: num_tokens_from_string(x))\n",
    "avg_report_length = report_lengths.mean()\n",
    "min_report_length = report_lengths.min()\n",
    "max_report_length = report_lengths.max()\n",
    "report_length_sum = report_lengths.sum()\n",
    "\n",
    "print(f\"Average report length: {avg_report_length:.2f} tokens\")\n",
    "print(f\"Minimum report length: {min_report_length} tokens\")\n",
    "print(f\"Maximum report length: {max_report_length} tokens\")\n",
    "print(f\"# The training dataset consists of: {report_length_sum} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abcbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(report_length_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model = 8.000   # Price for gpt-3.5-turbo per 1M tokens\n",
    "model = \"gpt-3.5-turbo\"\n",
    "price_per_epoch = (report_length_sum / 1000000) * price_model \n",
    "print(f\"Fine-tuning of {model} costs ~ ${price_per_epoch:.2f} per epoch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c914a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model = 0.0080   # Price for gpt-3.5-turbo per 1K tokens\n",
    "model = \"gpt-3.5-turbo\"\n",
    "price_per_epoch = (report_length_sum / 1000) * price_model \n",
    "print(f\"Fine-tuning of {model} costs ~ ${price_per_epoch:.2f} per epoch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['medical_specialty'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7431eda",
   "metadata": {},
   "source": [
    "### Fine-tuning data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b9bd9",
   "metadata": {},
   "source": [
    "We can now rearrange the dataset into the necessary format in order to start the fine tuning job.\n",
    "The format is as follows:\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Given the medical description report, classify it into one of these categories: [Cardiovascular / Pulmonary, Gastroenterology, Neurology, Radiology, Surgery]\"}, {\"role\": \"user\", \"content\": \"Medical Report\"}, {\"role\": \"assistant\", \"content\": \"The medical specialty assigned to this report\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a0489e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Given the medical description report, classify it into one of these categories: \" + \\\n",
    "                 \"[Cardiovascular / Pulmonary, Gastroenterology, Neurology, Radiology, Surgery]\"\n",
    "\n",
    "\n",
    "# print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,5):\n",
    "    print(train[\"report\"].iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87ce0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                              {\"role\": \"user\", \"content\": train[\"report\"].iloc[0]},\n",
    "                              {\"role\": \"assistant\", \"content\": train[\"medical_specialty\"].iloc[0]}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b671b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd89b94",
   "metadata": {},
   "source": [
    "Let's write a script that converts the dataframe into this format and stores everything as a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf1d87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_format(df):\n",
    "    formatted_data = []\n",
    "    \n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        entry = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                              {\"role\": \"user\", \"content\": row[\"report\"]},\n",
    "                              {\"role\": \"assistant\", \"content\": row[\"medical_specialty\"]}]}\n",
    "\n",
    "        formatted_data.append(entry)\n",
    "\n",
    "    return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8c47cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_to_format(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bce32",
   "metadata": {},
   "source": [
    "Let's dump this list of dictionaries into the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3835b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fine_tuning_data.jsonl', 'w') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effe23c",
   "metadata": {},
   "source": [
    "### Val Data\n",
    "Let's perform the same operation for the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00c1cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = df_to_format(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "493ffbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fine_tuning_data_val.jsonl', 'w') as f:\n",
    "    for entry in val_data:\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2855e1c",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "Before starting the training process, we should check if any input exceeds the maximum of 4096 tokens. Additionally, let's make sure that there are no empty reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01bb98dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_tokens(prompt):\n",
    "    prompt_text = \" \".join([content[\"content\"] for content in element[\"messages\"]])\n",
    "    tokens = num_tokens_from_string(prompt_text)\n",
    "    if tokens > 4000: # according to https://platform.openai.com/docs/guides/fine-tuning/token-limits\n",
    "        print(f\"Prompt {prompt} exceeds token limit!\")\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def check_prompt(prompt):\n",
    "    \n",
    "    if len(prompt[\"messages\"][1][\"content\"]):\n",
    "        if len(prompt[\"messages\"][2][\"content\"]):\n",
    "            return True\n",
    "    print(f\"Prompt {prompt} is missing data!\")\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ce613",
   "metadata": {},
   "source": [
    "We can now read the jsonl file and check each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "770817a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fine_tuning_data.jsonl', 'r') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "819fe86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    assert check_num_tokens(element) and check_prompt(element)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb6330a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fine_tuning_data_val.jsonl', 'r') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "for element in dataset:\n",
    "    assert check_num_tokens(element) and check_prompt(element)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b8459",
   "metadata": {},
   "source": [
    "Great! There are no violations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff47b2",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it's time to start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "print(\"*\" * 10 )\n",
    "print(OPENAI_API_KEY)\n",
    "print(\"*\" * 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6027b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "# openai.organization = \"org-FWKNS6AR0jCIQjL9hXW7WLCS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009cf97",
   "metadata": {},
   "source": [
    "At first we need to upload the fine-tuning data to openai using the **File.create** [method](https://platform.openai.com/docs/api-reference/files/create) to which you need to pass the binary file object and a purpose (fine-tuning in our case). If you pass \"fine-tuning\" as purpose, openai validates the file structure once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3e6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_upload_response = openai.File.create(\n",
    "  file=open(\"fine_tuning_data.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "File.retrieve(file-VoGYIusGVgG23AK30LpPPgGH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63352817",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_upload_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ea56b",
   "metadata": {},
   "source": [
    "Uploading the File object might take a while.\n",
    "You can navigate to https://platform.openai.com/files to check if your file has been processed.\n",
    "Alternativeley, you can use **File.retrieve(file_id)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.File.retrieve(file_upload_response[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ded300",
   "metadata": {},
   "source": [
    "Perform the same steps for the val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8fee50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_upload_response_val = openai.File.create(\n",
    "  file=open(\"fine_tuning_data_val.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.File.retrieve(file_upload_response_val[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3b0eb",
   "metadata": {},
   "source": [
    "Now it's time to start the [training process](https://platform.openai.com/docs/api-reference/fine-tuning/create):\n",
    "\n",
    "To start the training routine we can call FineTuningJob.create which accepts the following arguments:\n",
    "- object\n",
    "- id\n",
    "- model\n",
    "- created_at\n",
    "- fine_tuned_model\n",
    "- organization_id\n",
    "- result_files\n",
    "- status\n",
    "- validation_file\n",
    "- training_file\n",
    "\n",
    "Only *model* and *training_file* are required, the remaining arguments are optional.\n",
    "You can specify the number of epochs using the hyperparameter argument. Currently *n_epochs* is the only hyperparameter available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "58589286",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_response = openai.FineTuningJob.create(training_file=file_upload_response[\"id\"],\n",
    "                            model=\"gpt-3.5-turbo\",\n",
    "                            hyperparameters={\"n_epochs\": 1},\n",
    "                            validation_file = file_upload_response_val[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5afb8f",
   "metadata": {},
   "source": [
    "To obtain the log, you can use *FineTuningJob.list_events* to which you pass the job id and a limit if you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16510ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_response[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b37e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTuningJob.list_events(id=\"ftjob-0ZB6FD70DnweK1F6euj03SNg\", limit=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb2307",
   "metadata": {},
   "source": [
    "### Plot losses\n",
    "We can use *FineTuningJob.list_events* to obtain all event data and plot the training metrics.\n",
    "Note that if you do not pass a limit, openai will not automatically grab all data. Thus, it's best to pass a large limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "048d90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_event = openai.FineTuningJob.list_events(id=\"ftjob-0ZB6FD70DnweK1F6euj03SNg\", limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e40669e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for item in train_event[\"data\"]:\n",
    "    train_data = item[\"data\"]\n",
    "    if train_data and \"train_loss\" in train_data:\n",
    "        \n",
    "        # As the event list returns the most current event at first, we don't want to append but insert\n",
    "        train_loss.insert(0, train_data[\"train_loss\"])\n",
    "        val_loss.insert(0, train_data[\"valid_loss\"])\n",
    "        train_acc.insert(0, train_data[\"train_mean_token_accuracy\"])\n",
    "        val_acc.insert(0, train_data[\"valid_mean_token_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fc9fc",
   "metadata": {},
   "source": [
    "## Application\n",
    "To use the fine-tuned model, we just need to pass it to *ChatCompletion.create* and proceed as usual.\n",
    "You can grab the model name either via the openai [fine-tuning dashboard](https://platform.openai.com/finetune/) or using *openai.FineTuningJob.retrieve(id)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTuningJob.retrieve(\"ftjob-0ZB6FD70DnweK1F6euj03SNg\")[\"fine_tuned_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"report\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = \"ft:gpt-3.5-turbo-0613:pierian-training::8IjdNdPL\", \n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": test[\"report\"].iloc[1]}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"medical_specialty\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7be5c4",
   "metadata": {},
   "source": [
    "Let's loop over the test data and count how many reports are classified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b18d94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_report(report):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "                    model = \"ft:gpt-3.5-turbo-0613:pierian-training::8IjdNdPL\", \n",
    "                    # model = \"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": report}\n",
    "                          ]\n",
    "                        )\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8915b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "ground_truth_classes = []\n",
    "for line in test.iterrows():\n",
    "    report, specialty = line[1][\"report\"], line[1][\"medical_specialty\"]\n",
    "    ground_truth_classes.append(specialty.strip())  # in case of any trailing\n",
    "    prediction = classify_report(report)\n",
    "    predicted_classes.append(prediction.choices[0].message[\"content\"].strip())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4877bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59996277",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(predicted_classes) == np.array(ground_truth_classes)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8eaf1c",
   "metadata": {},
   "source": [
    "### Comparison to gpt-3.5-turbo\n",
    "Let's compare how our model works compared to the standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9a17591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def classify_report_baseline(report):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                        model = \"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                {\"role\": \"user\", \"content\": report}\n",
    "                              ],\n",
    "                        temperature=0\n",
    "                            )\n",
    "    except openai.error.APIConnectionError:  # To retry if openai loses the connection\n",
    "        time.sleep(10)\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                model = \"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": report}\n",
    "                      ],\n",
    "                temperature=0\n",
    "                    )\n",
    "\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = []\n",
    "ground_truth_classes = []\n",
    "for line in tqdm(test.iterrows()):\n",
    "    report, specialty = line[1][\"report\"], line[1][\"medical_specialty\"]\n",
    "    ground_truth_classes.append(specialty.strip())  # in case of any trailing\n",
    "    prediction = classify_report_baseline(report)\n",
    "    predicted_classes.append(prediction.choices[0].message[\"content\"].strip())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(predicted_classes) == np.array(ground_truth_classes)).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
